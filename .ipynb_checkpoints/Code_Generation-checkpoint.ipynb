{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e89eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import os, time\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c4a638",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a68cf0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"Your Token\"\n",
    "temperature_set= 1\n",
    "model_name = \"gpt-3.5-turbo-instruct\"\n",
    "\n",
    "Dataset_directory = 'Directory'\n",
    "Output_directory = 'Directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3f3908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(code):\n",
    "    \"\"\"\n",
    "    Extracts the code snippet enclosed within triple backticks from the given string.\n",
    "\n",
    "    The function searches for code wrapped with markdown-style triple backticks \n",
    "    and returns the content within those backticks. If the content has a language \n",
    "    specifier (like \"python\"), it removes the specifier.\n",
    "\n",
    "    Parameters:\n",
    "    - code (str): The input string which may contain a code snippet enclosed in triple backticks.\n",
    "\n",
    "    Returns:\n",
    "    - str: The extracted code snippet without the language specifier. Returns None if no code snippet is found.\n",
    "    \"\"\"\n",
    "    function = None\n",
    "    match = re.search('```(.*?)```', code, re.DOTALL)\n",
    "    if match:\n",
    "        function = match.group(1)\n",
    "        function = function.replace(\"python\",\"\")\n",
    "    return function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe7cd7",
   "metadata": {},
   "source": [
    "# Loading the PythonSecurityEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47ab2dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total functions: 500\n"
     ]
    }
   ],
   "source": [
    "directory = Dataset_directory\n",
    "benchmark = []\n",
    "\n",
    "for item in os.listdir(directory):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    if os.path.isfile(item_path+'/text.txt'):\n",
    "        with open(item_path+'/text.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        code_dict = {'idx': item,\n",
    "            'text': text ,\n",
    "            }\n",
    "\n",
    "        benchmark.append(code_dict)\n",
    "print('Total functions: ' + str(len(benchmark)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a08b6",
   "metadata": {},
   "source": [
    "# Loading the LLMSecEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e59b951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "with open(Dataset_directory, 'r') as file:\n",
    "    LLMSecEval = json.load(file)\n",
    "    \n",
    "for index, item in enumerate(LLMSecEval):\n",
    "    code_dict = {'idx': index + 1,\n",
    "                'text': item['NL Prompt'].replace('<language>','Python') ,\n",
    "                }\n",
    "    benchmark.append(code_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74830760",
   "metadata": {},
   "source": [
    "# Loading the SecurityEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d520ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "with open(Dataset_directory, 'r') as file:\n",
    "    SecurityEval = list(file)\n",
    "for index, prompt in enumerate(SecurityEval):\n",
    "    problem = json.loads(prompt)\n",
    "    code_dict = {'idx': index + 1,\n",
    "                'text': problem['Prompt'] ,\n",
    "                }\n",
    "    benchmark.append(code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed2ee407",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_code = '\\nPlease provide the answer as a code snippet enclosed in triple back-ticks.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3c4eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89111f",
   "metadata": {},
   "source": [
    "# Generating the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83814ea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'benchmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(extract_code(response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext))\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 29\u001b[0m     executor\u001b[38;5;241m.\u001b[39mmap(process_prompt, \u001b[43mbenchmark\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'benchmark' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import concurrent.futures\n",
    "\n",
    "def process_prompt(prompt):\n",
    "    \n",
    "    text = prompt['text'] + format_code\n",
    "    \n",
    "    directory = Output_directory + '/' + str(prompt['idx'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Generate the code\n",
    "    response = openai.Completion.create(\n",
    "        model=model_name,\n",
    "        prompt=[text],\n",
    "        temperature=temperature_set,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with open(directory + '/' + 'generate_code.py', 'w+') as file:\n",
    "        file.write(extract_code(response.choices[0].text))\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(process_prompt, benchmark)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
