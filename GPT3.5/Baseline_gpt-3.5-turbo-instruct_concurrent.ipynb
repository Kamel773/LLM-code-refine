{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aceba8-3bb0-4ec9-8aa4-259b071c1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import os, time\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import os\n",
    "import concurrent.futures\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f17110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"Your Token\"\n",
    "temperature_set = 1\n",
    "model_name = \"gpt-3.5-turbo-instruct\"\n",
    "directory = 'Directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b644195-5e7a-4b37-b8c4-589033c7e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bandit_Feedback(path):\n",
    "    \"\"\"\n",
    "    Analyzes a given Python code using the Bandit tool to identify security vulnerabilities.\n",
    "\n",
    "    This function writes the provided code to a temporary file, runs the Bandit security\n",
    "    linter tool on the file, and extracts the security issues identified by Bandit. The\n",
    "    function returns a list of these issues, with each issue being represented as a list\n",
    "    containing the test name, issue text, and the problematic code snippet.\n",
    "\n",
    "    Parameters:\n",
    "    - code (str): The Python code to be analyzed for security vulnerabilities.\n",
    "\n",
    "    Returns:\n",
    "    - list[list[str]]: A list of security issues, where each issue is represented as a list\n",
    "                       containing the test name, issue text, and the problematic code snippet.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = path\n",
    "\n",
    "    result = subprocess.run([\"bandit\", \"-r\", \"-f\", \"json\", file_name], capture_output=True, text=True)\n",
    "    output_json = json.loads(result.stdout)\n",
    "    Feedback = []\n",
    "    for issue in output_json['results']:\n",
    "        Feedback.append([issue['test_name'],issue['issue_text'],issue['code']])\n",
    "\n",
    "    return Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc802d2b-edd5-46c5-8af5-1a1661da1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_code(code):\n",
    "    \"\"\"\n",
    "    Extracts Python code enclosed between markdown code block delimiters.\n",
    "    \n",
    "    This function extracts Python code that is wrapped between markdown code block \n",
    "    delimiters (i.e., ```python ... ```). If there's no valid Python code block, or \n",
    "    if the extracted code has syntax errors, the function returns None. If valid \n",
    "    code is found, it returns the code as a string.\n",
    "    \n",
    "    Parameters:\n",
    "    - code (str): The string containing the potential markdown-wrapped Python code.\n",
    "\n",
    "    Returns:\n",
    "    - str or None: Extracted Python code if it's valid and has no syntax errors, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    function = None\n",
    "    match = re.search('```(.*?)```', code, re.DOTALL)\n",
    "    if match:\n",
    "        function = match.group(1)\n",
    "        function = function.replace(\"python\",\"\")\n",
    "\n",
    "    if function != None:\n",
    "        if len(Check_Syntax(function)) != 0:\n",
    "            return None\n",
    "        \n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a1b42d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Is_Fixed(code):\n",
    "    random_integer = random.randint(1, 100000000)\n",
    "    file_name = '/Directory/temp/' + str(random_integer) + 'temp.py'\n",
    "\n",
    "    with open(file_name, 'w+') as file:\n",
    "        file.write(code)\n",
    "        \n",
    "    result = subprocess.run([\"bandit\", \"-r\", \"-f\", \"json\", file_name], capture_output=True, text=True)\n",
    "    output_json = json.loads(result.stdout)\n",
    "    Feedback = []\n",
    "    for issue in output_json['results']:\n",
    "        Feedback.append([issue['issue_text'],issue['code']])\n",
    "\n",
    "    os.remove(file_name)\n",
    "    return len(Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "902a9dd7-f258-4915-be9b-c94279b7b681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Syntax(code):\n",
    "    \"\"\"\n",
    "    Checks the syntax of the given Python code for vulnerabilities using Bandit.\n",
    "\n",
    "    This function writes the provided Python code to a temporary file and runs the Bandit\n",
    "    tool on it to detect any security vulnerabilities. The function returns any errors\n",
    "    detected in the code in JSON format.\n",
    "\n",
    "    Parameters:\n",
    "    - code (str): The Python code to be checked for vulnerabilities.\n",
    "\n",
    "    Returns:\n",
    "    - list[dict]: A list of errors detected by Bandit in the given Python code.\n",
    "                  Each dictionary in the list contains details about a specific error.\n",
    "    \"\"\"\n",
    "    random_integer = random.randint(1, 100000000)\n",
    "    file_name = '/Directory/temp/' + str(random_integer) + 'temp.py'\n",
    "\n",
    "    with open(file_name, 'w+') as file:\n",
    "        file.write(code)\n",
    "\n",
    "    result = subprocess.run([\"bandit\", \"-r\", \"-f\", \"json\", file_name], capture_output=True, text=True)\n",
    "    output_json = json.loads(result.stdout)\n",
    "    os.remove(file_name)\n",
    "\n",
    "    return output_json['errors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a24983a-ead1-420e-8648-36269bd0418c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bandit(path):\n",
    "    \"\"\"\n",
    "    Analyze the given Python code using the Bandit tool for security vulnerabilities.\n",
    "\n",
    "    This function writes the given code to a temporary file, runs the Bandit tool\n",
    "    on the file to identify security vulnerabilities, and then parses the results \n",
    "    into a structured format for further use.\n",
    "\n",
    "    Parameters:\n",
    "    - code (str): The Python code string to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    int: The number of identified vulnerabilities in the provided code.\n",
    "    \"\"\"\n",
    "    file_name = path\n",
    "\n",
    "    result = subprocess.run([\"bandit\", \"-r\", \"-f\", \"json\", file_name], capture_output=True, text=True)\n",
    "    output_json = json.loads(result.stdout)\n",
    "    Feedback = []\n",
    "    for issue in output_json['results']:\n",
    "        Feedback.append([issue['issue_text'],issue['code']])\n",
    "\n",
    "    return len(Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d406329-1f67-43d7-8ff4-f446a921576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_code = '\\n#### Please provide the answer as a code snippet enclosed in triple back-ticks.'\n",
    "format_strategies = '\\n#### Please provide each strategy in triple back-ticks.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3e53a-2b04-416c-bd8d-c791c986c531",
   "metadata": {},
   "source": [
    "# Direct prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ab7ee20",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Direct_prompting(item):\n",
    "    item_path = os.path.join(directory, item)\n",
    "\n",
    "    file_path_refine = item_path + '/Direct_prompting.py'\n",
    "    if os.path.exists(file_path_refine):\n",
    "        return\n",
    "        \n",
    "    with open(item_path + '/generate_code.py', 'r') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    if run_bandit(item_path + '/generate_code.py') == 0:  # check if the function has no bug\n",
    "        return\n",
    "\n",
    "    text = '''#### Does the provided function have security issue? if yes, please fix the issue.\\n## Python function:\\n''' + '```' + code + '```' + format_code\n",
    "    response = openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[text],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    self_refine_code = extract_code(response.choices[0].text)\n",
    "    if self_refine_code is not None:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(self_refine_code)\n",
    "    else:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(code)\n",
    "\n",
    "items = os.listdir(directory)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(Direct_prompting, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0298ab00-fb78-4b41-a644-7f95886f1d57",
   "metadata": {},
   "source": [
    "# Self-Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c01590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "def Self_Debug(item):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    \n",
    "    file_path_refine = os.path.join(item_path, 'Self_Debug.py')\n",
    "    if os.path.exists(file_path_refine):\n",
    "        return\n",
    "\n",
    "    with open(os.path.join(item_path, 'generate_code.py'), 'r') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    if run_bandit(item_path + '/generate_code.py') == 0: \n",
    "        return\n",
    "\n",
    "    feedback_prompt = '##### Explaining the provided function for security debugging.\\n '\n",
    "    feedback_message = f\"{feedback_prompt}\\n\\n### Python function\\n\\n```{code}```\\n\"\n",
    "    \n",
    "    Code_explanation = openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    Feedback = Code_explanation.choices[0].text\n",
    "\n",
    "    feedback_prompt = '##### Based on the code explanation, please fix the security issues:\\n'\n",
    "    feedback_message = f\"{feedback_prompt}{Feedback}\\n\\n### Python function\\n\\n```{code}```\\n{format_code}\"\n",
    "\n",
    "    response_with_feedback_message= openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    self_refine_code = extract_code(response_with_feedback_message.choices[0].text)\n",
    "    \n",
    "\n",
    "    if self_refine_code is not None:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(self_refine_code)\n",
    "    else:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(code)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(Self_Debug, os.listdir(directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21d7e05-7b50-4393-a28a-c09ae595fb99",
   "metadata": {},
   "source": [
    "# Directly giving bandit feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eeeaa36-06cc-4711-b90b-cd0c136bafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_feedback(item):\n",
    "    item_path = os.path.join(directory, item)    \n",
    "\n",
    "    file_path_refine = item_path + '/Directly_giving_bandit_feedback.py'\n",
    "    if os.path.exists(file_path_refine):\n",
    "        return\n",
    "        \n",
    "    with open(item_path + '/generate_code.py', 'r') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    if run_bandit(item_path + '/generate_code.py') == 0:\n",
    "        return\n",
    "\n",
    "    Feedback_from_Bandit = Bandit_Feedback(item_path + '/generate_code.py')\n",
    "    feedback_details = \"\\n### The feedback from static code analysis:\\n\" +  \" \".join([f\"{idx}- {item[0]}\\n In the following lines:\\n{item[1]}\" for idx, item in enumerate(Feedback_from_Bandit, start=1)])\n",
    "\n",
    "    feedback_prompt = '##### Based on the feedback provided, please fix the following security issues:\\n'\n",
    "    feedback_message = f\"{feedback_prompt}{feedback_details}\\n\\n### Python function\\n\\n```{code}```\\n{format_code}\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    self_refine_code = extract_code(response.choices[0].text)\n",
    "    \n",
    "    if self_refine_code is not None:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(self_refine_code)\n",
    "    else:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(code)\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(bandit_feedback, os.listdir(directory))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265db93",
   "metadata": {},
   "source": [
    "# Verbalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94df898",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Verbalization(item):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    \n",
    "    file_path_refine = item_path + '/Verbalization.py'\n",
    "    if os.path.exists(file_path_refine):\n",
    "        return\n",
    "    \n",
    "    with open(item_path + '/generate_code.py', 'r') as f:\n",
    "        code = f.read()\n",
    "\n",
    "    if run_bandit(item_path + '/generate_code.py') == 0:  # check if the function has no bug\n",
    "        return\n",
    "\n",
    "\n",
    "    Feedback_from_Bandit = Bandit_Feedback(item_path + '/generate_code.py')\n",
    "    feedback_details = \" \".join([f\"{idx}{' >> Issue: ['} {item[0]}{'] '}{item[1]}\\n In the following lines:\\n{item[2]}\" for idx, item in enumerate(Feedback_from_Bandit, start=1)])\n",
    "\n",
    "    feedback_prompt = '##### verbalizing the feedback as an input to language model:\\n Test results:\\n '\n",
    "    feedback_message = f\"{feedback_prompt}{feedback_details}\\n\\n### Python function\\n\\n```{code}```\\n\"\n",
    "\n",
    "    response_with_feedback_message = openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    Feedback = response_with_feedback_message.choices[0].text\n",
    "\n",
    "    verbalizing_feedback_prompt = '##### Based on the feedback provided, please fix the security issues:\\n'\n",
    "    verbalizing_feedback_message = f\"{verbalizing_feedback_prompt}{Feedback}\\n{'Test results:'}\\n{feedback_details}\\n\\n### Python function\\n\\n```{code}```\\n{format_code}\"\n",
    "\n",
    "    response_with_feedback_message= openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[verbalizing_feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    self_refine_code = extract_code(response_with_feedback_message.choices[0].text)\n",
    "    \n",
    "\n",
    "    if self_refine_code is not None:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "            file.write(self_refine_code)\n",
    "    else:\n",
    "        with open(file_path_refine, 'w+') as file:\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    executor.map(Verbalization, os.listdir(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774611c4",
   "metadata": {},
   "source": [
    "# Feedback-Driven Solution Synthesis (FDSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba89a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def FDSS(item):\n",
    "    item_path = os.path.join(directory, item)\n",
    "    \n",
    "    file_path_refine = item_path + '/strategies.py'\n",
    "    if os.path.exists(file_path_refine):\n",
    "        return\n",
    "    \n",
    "    with open(item_path + '/generate_code.py', 'r') as f:\n",
    "        code = f.read()\n",
    "        \n",
    "    if run_bandit(item_path + '/generate_code.py') == 0: # check if the function has no bug\n",
    "        return\n",
    "    \n",
    "    Feedback_from_Bandit = Bandit_Feedback(item_path + '/generate_code.py')\n",
    "    feedback_details = \" \".join([f\"{idx}{' >> Issue: ['} {item[0]}{'] '}{item[1]}\\n In the following lines:\\n{item[2]}\" for idx, item in enumerate(Feedback_from_Bandit, start=1)])\n",
    "\n",
    "    feedback_prompt = '##### verbalizing the feedback as an input to language model:\\n '\n",
    "    feedback_message = f\"{feedback_prompt}{feedback_details}\\n\\n### Python function\\n\\n```{code}```\\n\"\n",
    "\n",
    "    response_with_feedback_message = openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[feedback_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    Feedback = response_with_feedback_message.choices[0].text\n",
    "\n",
    "\n",
    "    K_search_prompt = '##### provide three different strategies to fix the security issues:\\n Test results:\\n '\n",
    "    format_strategies = '\\n#### Please provide each strategy in 1), 2) and 3)'\n",
    "    K_search_message = f\"{K_search_prompt}{Feedback}\\n{'Test results:'}\\n{feedback_details}\\n\\n### Python function\\n\\n{code}\\n{format_strategies}\"\n",
    "\n",
    "\n",
    "    \n",
    "    response_K_search_message_message= openai.Completion.create(\n",
    "      model=model_name,\n",
    "      prompt=[K_search_message],\n",
    "      temperature=temperature_set,\n",
    "      max_tokens=2000,\n",
    "      n=1,\n",
    "      logprobs=1,\n",
    "      frequency_penalty=0.0,\n",
    "      presence_penalty=0.0,\n",
    "    )\n",
    "\n",
    "    K_search_feedback = response_K_search_message_message.choices[0].text\n",
    "    strategies = re.split(r'\\d\\)', K_search_feedback)[1:] \n",
    "    \n",
    "    for strategy in strategies:\n",
    "\n",
    "        Iterative = 1\n",
    "        fixed = False\n",
    "        while(Iterative != MAX_Iterative):\n",
    "            feedback_prompt = '##### Based on the feedback provided, please fix the security issues:\\n'\n",
    "            feedback_message = f\"{feedback_prompt}{Feedback}\\n{'Test results:'}\\n{feedback_details}\\n{strategy}\\n### Python function\\n\\n```{code}```\\n\"\n",
    "            \n",
    "            response_K_search_message_message= openai.Completion.create(\n",
    "                  model=model_name,\n",
    "                  prompt=[feedback_message],\n",
    "                  temperature=temperature_set,\n",
    "                  max_tokens=2000,\n",
    "                  n=1,\n",
    "                  logprobs=1,\n",
    "                  frequency_penalty=0.0,\n",
    "                  presence_penalty=0.0,\n",
    "            )\n",
    "\n",
    "            self_refine_code = extract_code(response_K_search_message_message.choices[0].text)\n",
    "\n",
    "            if self_refine_code is not None:\n",
    "                Feedback_from_Bandit = Is_Fixed(self_refine_code) \n",
    "                with open(file_path_refine, 'w+') as file:\n",
    "                    file.write(self_refine_code)\n",
    "                if len(Feedback_from_Bandit) == 0:\n",
    "                    fixed = True\n",
    "                    break\n",
    "            else:\n",
    "                with open(file_path_refine, 'w+') as file:\n",
    "                    file.write(code)\n",
    "            \n",
    "            Iterative = Iterative + 1\n",
    "        if fixed == True:\n",
    "            break\n",
    "\n",
    "MAX_Iterative = 3\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    executor.map(FDSS, os.listdir(directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df414d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
